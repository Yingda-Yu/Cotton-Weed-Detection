#!/usr/bin/env python3
"""
å®Œæ•´çš„æ•°æ®æ¸…æ´—å’Œæ€§èƒ½å¯¹æ¯”æµç¨‹

è‡ªåŠ¨å®Œæˆï¼š
1. æ¸…æ´—æ•°æ®é›†æ ‡æ³¨
2. è½¬æ¢å›YOLOæ ¼å¼
3. å¤åˆ¶å›¾ç‰‡æ–‡ä»¶
4. ä½¿ç”¨æ¸…æ´—åçš„æ•°æ®è®­ç»ƒæ¨¡å‹
5. å¯¹æ¯”æ¸…æ´—å‰åçš„æ€§èƒ½

âš ï¸ æ³¨æ„ï¼šæ‰€æœ‰æ“ä½œéƒ½ä¸ä¼šä¿®æ”¹åŸå§‹æ•°æ®é›†
"""

import json
import yaml
import shutil
import subprocess
import sys
from pathlib import Path
from datetime import datetime
import argparse

# ============================================================================
# é…ç½®å‚æ•°
# ============================================================================

# æ¸…æ´—å‚æ•°ï¼ˆè®­ç»ƒé›†ï¼‰
QUALITY_REPORT = "quality_report_train.json"
PREDICTIONS_FILE = "predictions_train_coco.json"
CLEANED_ANNOTATIONS = "cleaned_train_annotations.json"

# æ¸…æ´—é˜ˆå€¼
LOCATION_THRESHOLD = 0.7
LABEL_THRESHOLD = 0.8
MISSING_THRESHOLD = 0.5

# æ•°æ®é›†è·¯å¾„ï¼ˆæ¸…æ´—è®­ç»ƒé›†ï¼ŒéªŒè¯é›†ä¿æŒåŸå§‹ï¼‰
ORIGINAL_TRAIN_DIR = "train"
CLEANED_TRAIN_DIR = "cleaned_train"

# è®­ç»ƒé…ç½®
BASELINE_MODEL = "runs/detect/yolov8n_baseline_new/weights/best.pt"  # ç”¨äºç”Ÿæˆé¢„æµ‹çš„åŸºçº¿æ¨¡å‹
EPOCHS = 10  # è®­ç»ƒè½®æ•°
BATCH_SIZE = 8  # å‡å°batch sizeé¿å…å†…å­˜é—®é¢˜
RUN_NAME_CLEANED = "yolov8n_cleaned_new"  # æ¸…æ´—åè®­ç»ƒçš„runåç§°

# è¾“å‡ºæ–‡ä»¶
COMPARISON_REPORT = "cleaning_comparison_report.json"


def print_section(title):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70)


def step1_clean_dataset():
    """æ­¥éª¤1: æ¸…æ´—æ•°æ®é›†"""
    print_section("æ­¥éª¤1: è‡ªåŠ¨æ¸…æ´—æ•°æ®é›†æ ‡æ³¨")
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    if not Path(QUALITY_REPORT).exists():
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è´¨é‡æŠ¥å‘Šæ–‡ä»¶: {QUALITY_REPORT}")
        print("   è¯·å…ˆè¿è¡Œ: python run_label_quality_analysis.py --model <model_path> --split train")
        return False
    
    if not Path(PREDICTIONS_FILE).exists():
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°é¢„æµ‹ç»“æœæ–‡ä»¶: {PREDICTIONS_FILE}")
        print("   è¯·å…ˆè¿è¡Œ: python run_label_quality_analysis.py --model <model_path> --split train")
        return False
    
    # è¿è¡Œæ¸…æ´—è„šæœ¬
    print(f"è¿è¡Œæ¸…æ´—è„šæœ¬...")
    print(f"  è´¨é‡æŠ¥å‘Š: {QUALITY_REPORT}")
    print(f"  é¢„æµ‹ç»“æœ: {PREDICTIONS_FILE}")
    print(f"  è¾“å‡ºæ–‡ä»¶: {CLEANED_ANNOTATIONS}")
    print(f"\n  æ¸…æ´—é˜ˆå€¼:")
    print(f"    Location: {LOCATION_THRESHOLD}")
    print(f"    Label: {LABEL_THRESHOLD}")
    print(f"    Missing: {MISSING_THRESHOLD}")
    
    try:
        from tools.clean_dataset import clean_dataset
        
        cleaned_data = clean_dataset(
            quality_report_file=QUALITY_REPORT,
            predictions_file=PREDICTIONS_FILE,
            output_file=CLEANED_ANNOTATIONS,
            location_score_threshold=LOCATION_THRESHOLD,
            label_score_threshold=LABEL_THRESHOLD,
            missing_score_threshold=MISSING_THRESHOLD
        )
        
        # ç»Ÿè®¡æ¸…æ´—ç»“æœ
        original_count = len([ann for ann in json.load(open(QUALITY_REPORT))["annotations"] 
                             if ann.get("id", 0) >= 0])
        cleaned_count = len(cleaned_data["annotations"])
        
        print(f"\nâœ… æ¸…æ´—å®Œæˆ!")
        print(f"   åŸå§‹æ ‡æ³¨æ•°: {original_count}")
        print(f"   æ¸…æ´—åæ ‡æ³¨æ•°: {cleaned_count}")
        print(f"   å‡€å˜åŒ–: {cleaned_count - original_count}")
        
        return {
            "original_annotations": original_count,
            "cleaned_annotations": cleaned_count,
            "net_change": cleaned_count - original_count
        }
        
    except Exception as e:
        print(f"âŒ æ¸…æ´—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def step2_convert_to_yolo():
    """æ­¥éª¤2: è½¬æ¢å›YOLOæ ¼å¼"""
    print_section("æ­¥éª¤2: è½¬æ¢æ¸…æ´—åçš„æ ‡æ³¨ä¸ºYOLOæ ¼å¼")
    
    if not Path(CLEANED_ANNOTATIONS).exists():
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¸…æ´—åçš„æ ‡æ³¨æ–‡ä»¶: {CLEANED_ANNOTATIONS}")
        return False
    
    try:
        from dataset.coco_to_yolo import coco_to_yolo
        
        output_labels_dir = coco_to_yolo(
            coco_file=CLEANED_ANNOTATIONS,
            split_dir=ORIGINAL_TRAIN_DIR,
            output_dir=CLEANED_TRAIN_DIR
        )
        
        # ç»Ÿè®¡è½¬æ¢ç»“æœ
        label_files = list(output_labels_dir.glob("*.txt"))
        print(f"\nâœ… è½¬æ¢å®Œæˆ!")
        print(f"   ç”Ÿæˆçš„æ ‡æ³¨æ–‡ä»¶æ•°: {len(label_files)}")
        print(f"   è¾“å‡ºç›®å½•: {output_labels_dir.absolute()}")
        
        return True
        
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def step3_copy_images():
    """æ­¥éª¤3: å¤åˆ¶å›¾ç‰‡æ–‡ä»¶"""
    print_section("æ­¥éª¤3: å¤åˆ¶å›¾ç‰‡æ–‡ä»¶åˆ°æ¸…æ´—åçš„æ•°æ®é›†")
    
    original_images_dir = Path(ORIGINAL_TRAIN_DIR) / "images"
    cleaned_images_dir = Path(CLEANED_TRAIN_DIR) / "images"
    
    if not original_images_dir.exists():
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°åŸå§‹å›¾ç‰‡ç›®å½•: {original_images_dir}")
        return False
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    cleaned_images_dir.mkdir(parents=True, exist_ok=True)
    
    # å¤åˆ¶å›¾ç‰‡
    image_files = list(original_images_dir.glob("*.jpg"))
    print(f"æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡")
    
    copied = 0
    for img_file in image_files:
        dest = cleaned_images_dir / img_file.name
        if not dest.exists():
            shutil.copy2(img_file, dest)
            copied += 1
    
    print(f"\nâœ… å¤åˆ¶å®Œæˆ!")
    print(f"   å¤åˆ¶å›¾ç‰‡æ•°: {copied}")
    print(f"   è¾“å‡ºç›®å½•: {cleaned_images_dir.absolute()}")
    
    return True


def step4_create_dataset_yaml():
    """æ­¥éª¤4: åˆ›å»ºæ¸…æ´—åçš„æ•°æ®é›†é…ç½®æ–‡ä»¶"""
    print_section("æ­¥éª¤4: åˆ›å»ºæ¸…æ´—åçš„æ•°æ®é›†é…ç½®")
    
    # è¯»å–åŸå§‹é…ç½®
    with open("dataset.yaml", 'r', encoding='utf-8') as f:
        original_config = yaml.safe_load(f)
    
    # åˆ›å»ºæ–°é…ç½®ï¼ˆä½¿ç”¨æ¸…æ´—åçš„è®­ç»ƒé›†ï¼ŒéªŒè¯é›†ä¿æŒåŸå§‹ï¼‰
    cleaned_config = original_config.copy()
    cleaned_config["train"] = f"{CLEANED_TRAIN_DIR}/images"
    cleaned_config["val"] = "val/images"  # ä¿æŒåŸå§‹éªŒè¯é›†ï¼Œç”¨äºçœŸå®è¯„ä¼°
    
    # ä¿å­˜æ–°é…ç½®
    cleaned_yaml = "dataset_cleaned.yaml"
    with open(cleaned_yaml, 'w', encoding='utf-8') as f:
        yaml.dump(cleaned_config, f, default_flow_style=False, allow_unicode=True)
    
    print(f"âœ… é…ç½®æ–‡ä»¶å·²åˆ›å»º: {cleaned_yaml}")
    print(f"   è®­ç»ƒé›†: {cleaned_config['train']} (æ¸…æ´—å)")
    print(f"   éªŒè¯é›†: {cleaned_config['val']} (åŸå§‹ï¼Œç”¨äºçœŸå®è¯„ä¼°)")
    
    return cleaned_yaml


def step5_train_with_cleaned_data(dataset_yaml):
    """æ­¥éª¤5: ä½¿ç”¨æ¸…æ´—åçš„æ•°æ®è®­ç»ƒæ¨¡å‹"""
    print_section("æ­¥éª¤5: ä½¿ç”¨æ¸…æ´—åçš„æ•°æ®è®­ç»ƒæ¨¡å‹")
    
    print(f"è®­ç»ƒé…ç½®:")
    print(f"  æ•°æ®é›†é…ç½®: {dataset_yaml}")
    print(f"  è®­ç»ƒè½®æ•°: {EPOCHS}")
    print(f"  æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"  Runåç§°: {RUN_NAME_CLEANED}")
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰è®­ç»ƒç»“æœ
    weights_path = Path(f"runs/detect/{RUN_NAME_CLEANED}/weights/best.pt")
    if weights_path.exists():
        print(f"\nâš ï¸  è­¦å‘Š: å‘ç°å·²å­˜åœ¨çš„è®­ç»ƒç»“æœ: {weights_path}")
        response = input("   æ˜¯å¦è·³è¿‡è®­ç»ƒï¼Œä½¿ç”¨å·²æœ‰æ¨¡å‹? (y/n): ").strip().lower()
        if response == 'y':
            print("   ä½¿ç”¨å·²æœ‰æ¨¡å‹...")
            return str(weights_path)
    
    # ä½¿ç”¨Ultralyticsè®­ç»ƒï¼ˆä¸ä½¿ç”¨3LCï¼Œå› ä¸ºæ¸…æ´—åçš„æ•°æ®ä¸åœ¨3LCä¸­ï¼‰
    try:
        from ultralytics import YOLO
        
        print(f"\nå¼€å§‹è®­ç»ƒ...")
        model = YOLO("yolov8n.pt")
        
        results = model.train(
            data=dataset_yaml,
            epochs=EPOCHS,
            batch=BATCH_SIZE,
            imgsz=640,
            name=RUN_NAME_CLEANED,
            project="runs/detect",
            device=0,
            workers=4
        )
        
        if weights_path.exists():
            print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
            print(f"   æ¨¡å‹æƒé‡: {weights_path.absolute()}")
            
            # æå–æœ€ä½³mAPï¼ˆå°è¯•å¤šç§æ–¹å¼ï¼‰
            best_map = None
            try:
                # æ–¹å¼1: ä»resultså¯¹è±¡è·å–
                if hasattr(results, 'results_dict'):
                    best_map = results.results_dict.get('metrics/mAP50(B)', None)
                # æ–¹å¼2: ä»results.csvè¯»å–
                if best_map is None:
                    results_csv = weights_path.parent.parent / "results.csv"
                    if results_csv.exists():
                        try:
                            import pandas as pd
                            df = pd.read_csv(results_csv)
                            if len(df) > 0:
                                last_row = df.iloc[-1]
                                for col in df.columns:
                                    if 'map50' in col.lower() or 'mAP50' in col:
                                        best_map = last_row.get(col, None)
                                        break
                        except:
                            # æ‰‹åŠ¨è§£æCSV
                            with open(results_csv, 'r') as f:
                                lines = f.readlines()
                                if len(lines) > 1:
                                    headers = lines[0].strip().split(',')
                                    last_line = lines[-1].strip().split(',')
                                    for i, h in enumerate(headers):
                                        if 'map50' in h.lower() or 'mAP50' in h:
                                            try:
                                                best_map = float(last_line[i])
                                            except:
                                                pass
                                            break
            except Exception as e:
                print(f"   è­¦å‘Š: æ— æ³•è‡ªåŠ¨æå–mAP: {e}")
            
            if best_map is not None:
                print(f"   æœ€ä½³mAP@0.5: {best_map:.4f}")
            else:
                print(f"   âš ï¸  æ— æ³•è‡ªåŠ¨æå–mAPï¼Œè¯·æ‰‹åŠ¨æŸ¥çœ‹è®­ç»ƒæ—¥å¿—")
                user_input = input("   è¯·è¾“å…¥æœ€ä½³mAP@0.5å€¼ï¼ˆç›´æ¥å›è½¦è·³è¿‡ï¼‰: ").strip()
                if user_input:
                    try:
                        best_map = float(user_input)
                    except ValueError:
                        best_map = None
            
            return {
                "weights_path": str(weights_path),
                "best_map": best_map,
                "epochs": EPOCHS
            }
        else:
            print(f"âŒ é”™è¯¯: è®­ç»ƒå®Œæˆä½†æ‰¾ä¸åˆ°æƒé‡æ–‡ä»¶: {weights_path}")
            return None
            
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def step6_get_baseline_performance():
    """æ­¥éª¤6: è·å–åŸºçº¿æ¨¡å‹æ€§èƒ½"""
    print_section("æ­¥éª¤6: è·å–åŸºçº¿æ¨¡å‹æ€§èƒ½")
    
    baseline_weights = Path(BASELINE_MODEL)
    if not baseline_weights.exists():
        print(f"âš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ°åŸºçº¿æ¨¡å‹: {BASELINE_MODEL}")
        print("   å°†ä½¿ç”¨è®­ç»ƒæ—¥å¿—ä¸­çš„ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰")
        return None
    
    # å°è¯•ä»è®­ç»ƒç»“æœç›®å½•è¯»å–metrics
    baseline_run_dir = baseline_weights.parent.parent
    results_file = baseline_run_dir / "results.csv"
    
    best_map = None
    
    if results_file.exists():
        try:
            # å°è¯•ä½¿ç”¨pandasè¯»å–
            try:
                import pandas as pd
                df = pd.read_csv(results_file)
                if len(df) > 0:
                    # è·å–æœ€åä¸€è¡Œçš„mAP
                    last_row = df.iloc[-1]
                    best_map = last_row.get('metrics/mAP50(B)', None)
                    if best_map is None or pd.isna(best_map):
                        # å°è¯•å…¶ä»–å¯èƒ½çš„åˆ—å
                        for col in df.columns:
                            if 'map50' in col.lower() or 'mAP50' in col:
                                best_map = last_row.get(col, None)
                                break
            except ImportError:
                # å¦‚æœæ²¡æœ‰pandasï¼Œæ‰‹åŠ¨è§£æCSV
                with open(results_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    if len(lines) > 1:
                        # ç¬¬ä¸€è¡Œæ˜¯è¡¨å¤´
                        headers = lines[0].strip().split(',')
                        # æœ€åä¸€è¡Œæ˜¯æ•°æ®
                        last_line = lines[-1].strip().split(',')
                        
                        # æŸ¥æ‰¾mAPåˆ—
                        map_col_idx = None
                        for i, h in enumerate(headers):
                            if 'map50' in h.lower() or 'mAP50' in h:
                                map_col_idx = i
                                break
                        
                        if map_col_idx is not None and map_col_idx < len(last_line):
                            try:
                                best_map = float(last_line[map_col_idx])
                            except ValueError:
                                pass
        except Exception as e:
            print(f"âš ï¸  æ— æ³•è¯»å–ç»“æœæ–‡ä»¶: {e}")
    
    if best_map is not None:
        print(f"âœ… åŸºçº¿æ€§èƒ½:")
        print(f"   æ¨¡å‹: {BASELINE_MODEL}")
        print(f"   æœ€ä½³mAP@0.5: {best_map:.4f}")
        
        return {
            "weights_path": str(baseline_weights),
            "best_map": float(best_map)
        }
    else:
        print("âš ï¸  æ— æ³•è‡ªåŠ¨è·å–åŸºçº¿æ€§èƒ½")
        print("   è¯·æ‰‹åŠ¨è¾“å…¥åŸºçº¿æ¨¡å‹çš„mAP@0.5å€¼ï¼Œæˆ–æŒ‰Enterè·³è¿‡")
        user_input = input("   åŸºçº¿mAP@0.5 (ç›´æ¥å›è½¦è·³è¿‡): ").strip()
        if user_input:
            try:
                best_map = float(user_input)
                return {
                    "weights_path": str(baseline_weights),
                    "best_map": best_map,
                    "source": "manual_input"
                }
            except ValueError:
                print("   è¾“å…¥æ— æ•ˆï¼Œè·³è¿‡")
        
        return None


def step7_compare_performance(baseline_perf, cleaned_perf, cleaning_stats):
    """æ­¥éª¤7: å¯¹æ¯”æ€§èƒ½"""
    print_section("æ­¥éª¤7: æ€§èƒ½å¯¹æ¯”åˆ†æ")
    
    comparison = {
        "timestamp": datetime.now().isoformat(),
        "cleaning_stats": cleaning_stats,
        "baseline": baseline_perf,
        "cleaned": cleaned_perf
    }
    
    if baseline_perf and cleaned_perf:
        baseline_map = baseline_perf.get("best_map", 0)
        cleaned_map = cleaned_perf.get("best_map", 0)
        
        improvement = cleaned_map - baseline_map
        improvement_pct = (improvement / baseline_map * 100) if baseline_map > 0 else 0
        
        comparison["improvement"] = {
            "absolute": improvement,
            "percentage": improvement_pct
        }
        
        print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
        print(f"   åŸºçº¿æ¨¡å‹ mAP@0.5: {baseline_map:.4f}")
        print(f"   æ¸…æ´—åæ¨¡å‹ mAP@0.5: {cleaned_map:.4f}")
        print(f"   ç»å¯¹æå‡: {improvement:+.4f}")
        print(f"   ç›¸å¯¹æå‡: {improvement_pct:+.2f}%")
        
        if improvement > 0:
            print(f"\n   âœ… æ¸…æ´—åæ€§èƒ½æå‡!")
        elif improvement < 0:
            print(f"\n   âš ï¸  æ¸…æ´—åæ€§èƒ½ä¸‹é™ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´é˜ˆå€¼")
        else:
            print(f"\n   â¡ï¸  æ€§èƒ½æ— æ˜æ˜¾å˜åŒ–")
    
    # ä¿å­˜å¯¹æ¯”æŠ¥å‘Š
    with open(COMPARISON_REPORT, 'w', encoding='utf-8') as f:
        json.dump(comparison, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {COMPARISON_REPORT}")
    
    return comparison


def main():
    """ä¸»æµç¨‹"""
    global BASELINE_MODEL, EPOCHS, LOCATION_THRESHOLD, LABEL_THRESHOLD, MISSING_THRESHOLD
    
    parser = argparse.ArgumentParser(
        description="å®Œæ•´çš„æ•°æ®æ¸…æ´—å’Œæ€§èƒ½å¯¹æ¯”æµç¨‹"
    )
    parser.add_argument(
        "--skip-cleaning",
        action="store_true",
        help="è·³è¿‡æ¸…æ´—æ­¥éª¤ï¼ˆå¦‚æœå·²ç»æ¸…æ´—è¿‡ï¼‰"
    )
    parser.add_argument(
        "--skip-training",
        action="store_true",
        help="è·³è¿‡è®­ç»ƒæ­¥éª¤ï¼ˆå¦‚æœå·²ç»è®­ç»ƒè¿‡ï¼‰"
    )
    parser.add_argument(
        "--baseline-model",
        type=str,
        default=BASELINE_MODEL,
        help="åŸºçº¿æ¨¡å‹è·¯å¾„ï¼ˆç”¨äºå¯¹æ¯”ï¼‰"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=EPOCHS,
        help="è®­ç»ƒè½®æ•°"
    )
    parser.add_argument(
        "--location-threshold",
        type=float,
        default=LOCATION_THRESHOLD,
        help="Locationä¿®å¤é˜ˆå€¼"
    )
    parser.add_argument(
        "--label-threshold",
        type=float,
        default=LABEL_THRESHOLD,
        help="Labelä¿®å¤é˜ˆå€¼"
    )
    parser.add_argument(
        "--missing-threshold",
        type=float,
        default=MISSING_THRESHOLD,
        help="Missingæ·»åŠ é˜ˆå€¼"
    )
    
    args = parser.parse_args()
    
    # æ›´æ–°å…¨å±€é…ç½®
    BASELINE_MODEL = args.baseline_model
    EPOCHS = args.epochs
    LOCATION_THRESHOLD = args.location_threshold
    LABEL_THRESHOLD = args.label_threshold
    MISSING_THRESHOLD = args.missing_threshold
    
    print("=" * 70)
    print("  å®Œæ•´æ•°æ®æ¸…æ´—å’Œæ€§èƒ½å¯¹æ¯”æµç¨‹")
    print("=" * 70)
    print(f"\né…ç½®:")
    print(f"  åŸºçº¿æ¨¡å‹: {BASELINE_MODEL}")
    print(f"  è®­ç»ƒè½®æ•°: {EPOCHS}")
    print(f"  æ¸…æ´—é˜ˆå€¼: Location={LOCATION_THRESHOLD}, Label={LABEL_THRESHOLD}, Missing={MISSING_THRESHOLD}")
    print(f"  è¾“å‡ºç›®å½•: {CLEANED_TRAIN_DIR}")
    
    results = {}
    
    # æ­¥éª¤1: æ¸…æ´—æ•°æ®é›†
    if not args.skip_cleaning:
        cleaning_stats = step1_clean_dataset()
        if cleaning_stats is None:
            print("\nâŒ æ¸…æ´—å¤±è´¥ï¼Œæµç¨‹ç»ˆæ­¢")
            return
        results["cleaning_stats"] = cleaning_stats
    else:
        print("\nâ­ï¸  è·³è¿‡æ¸…æ´—æ­¥éª¤")
        # å°è¯•è¯»å–å·²æœ‰çš„æ¸…æ´—ç»Ÿè®¡
        if Path(CLEANED_ANNOTATIONS).exists():
            with open(CLEANED_ANNOTATIONS, 'r', encoding='utf-8') as f:
                cleaned_data = json.load(f)
            results["cleaning_stats"] = {
                "cleaned_annotations": len(cleaned_data["annotations"])
            }
    
    # æ­¥éª¤2: è½¬æ¢å›YOLOæ ¼å¼
    if not Path(CLEANED_TRAIN_DIR).exists() or not list(Path(CLEANED_TRAIN_DIR).glob("labels/*.txt")):
        if not step2_convert_to_yolo():
            print("\nâŒ è½¬æ¢å¤±è´¥ï¼Œæµç¨‹ç»ˆæ­¢")
            return
    else:
        print("\nâ­ï¸  è·³è¿‡è½¬æ¢æ­¥éª¤ï¼ˆå·²å­˜åœ¨æ¸…æ´—åçš„æ ‡æ³¨ï¼‰")
    
    # æ­¥éª¤3: å¤åˆ¶å›¾ç‰‡
    cleaned_images_dir = Path(CLEANED_TRAIN_DIR) / "images"
    if not cleaned_images_dir.exists() or not list(cleaned_images_dir.glob("*.jpg")):
        if not step3_copy_images():
            print("\nâŒ å¤åˆ¶å›¾ç‰‡å¤±è´¥ï¼Œæµç¨‹ç»ˆæ­¢")
            return
    else:
        print("\nâ­ï¸  è·³è¿‡å¤åˆ¶å›¾ç‰‡æ­¥éª¤ï¼ˆå·²å­˜åœ¨ï¼‰")
    
    # æ­¥éª¤4: åˆ›å»ºæ•°æ®é›†é…ç½®
    dataset_yaml = step4_create_dataset_yaml()
    
    # æ­¥éª¤5: è®­ç»ƒæ¨¡å‹
    if not args.skip_training:
        cleaned_perf = step5_train_with_cleaned_data(dataset_yaml)
        if cleaned_perf is None:
            print("\nâŒ è®­ç»ƒå¤±è´¥ï¼Œæµç¨‹ç»ˆæ­¢")
            return
        results["cleaned_performance"] = cleaned_perf
    else:
        print("\nâ­ï¸  è·³è¿‡è®­ç»ƒæ­¥éª¤")
        # å°è¯•è¯»å–å·²æœ‰æ¨¡å‹
        weights_path = Path(f"runs/detect/{RUN_NAME_CLEANED}/weights/best.pt")
        if weights_path.exists():
            results["cleaned_performance"] = {
                "weights_path": str(weights_path),
                "note": "ä½¿ç”¨å·²æœ‰æ¨¡å‹"
            }
    
    # æ­¥éª¤6: è·å–åŸºçº¿æ€§èƒ½
    baseline_perf = step6_get_baseline_performance()
    if baseline_perf:
        results["baseline_performance"] = baseline_perf
    
    # æ­¥éª¤7: å¯¹æ¯”æ€§èƒ½
    comparison = step7_compare_performance(
        baseline_perf,
        results.get("cleaned_performance"),
        results.get("cleaning_stats")
    )
    
    # æœ€ç»ˆæ€»ç»“
    print("\n" + "=" * 70)
    print("  æµç¨‹å®Œæˆ!")
    print("=" * 70)
    print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  - æ¸…æ´—åçš„æ ‡æ³¨: {CLEANED_ANNOTATIONS}")
    print(f"  - æ¸…æ´—åçš„è®­ç»ƒé›†: {CLEANED_TRAIN_DIR}/")
    print(f"  - æ•°æ®é›†é…ç½®: dataset_cleaned.yaml")
    print(f"  - å¯¹æ¯”æŠ¥å‘Š: {COMPARISON_REPORT}")
    print(f"\né‡è¦è¯´æ˜:")
    print(f"  âœ… è®­ç»ƒé›†å·²æ¸…æ´—: {CLEANED_TRAIN_DIR}/")
    print(f"  âœ… éªŒè¯é›†ä¿æŒåŸå§‹: val/ (ç”¨äºçœŸå®è¯„ä¼°)")
    print(f"\nä¸‹ä¸€æ­¥:")
    print(f"  1. æŸ¥çœ‹å¯¹æ¯”æŠ¥å‘Š: {COMPARISON_REPORT}")
    print(f"  2. å¦‚æœæ€§èƒ½æå‡ï¼Œå¯ä»¥ç»§ç»­ä¼˜åŒ–æ¸…æ´—é˜ˆå€¼")
    print(f"  3. å¦‚æœæ€§èƒ½ä¸‹é™ï¼Œå¯ä»¥è°ƒæ•´é˜ˆå€¼æˆ–æ‰‹åŠ¨æ£€æŸ¥æ¸…æ´—ç»“æœ")
    print("=" * 70)


if __name__ == "__main__":
    main()

